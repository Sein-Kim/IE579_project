{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class multi_head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(multi_head, self).__init__()\n",
    "        self.A = nn.ModuleList([nn.Linear(32, 1) for _ in range(2)])\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for i in range(2):\n",
    "            nn.init.xavier_normal_(self.A[i].weight)\n",
    "            self.A[i].bias.data.fill_(0.0)\n",
    "    \n",
    "    def attn_summary(self, features):\n",
    "        features_attn = []\n",
    "        k = features[0].shape[0]\n",
    "        for i in range(2):\n",
    "            features_attn.append((self.A[i](features[i].squeeze())))\n",
    "        features_attn = F.softmax(torch.cat(features_attn), dim=-1).unsqueeze(1)\n",
    "        features = torch.cat(features)\n",
    "        features = (features * features)\n",
    "        \n",
    "        # k = features[i].shape[0]\n",
    "        print(k)\n",
    "        features = torch.cat((features[:k],features[k:]),axis=1)\n",
    "        \n",
    "        print(features.shape)\n",
    "        print(features)\n",
    "        # features = features.reshape(-1,64)\n",
    "        return features, features_attn\n",
    "\n",
    "# transposed_tensor = input_tensor.transpose(0, 1)\n",
    "\n",
    "# # Repeat each row of the transposed tensor\n",
    "# output_tensor = transposed_tensor.unsqueeze(1).expand(-1, len(input_tensor), -1).reshape(-1, len(input_tensor[0]))\n",
    "\n",
    "# print(output_tensor)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x1,x2,x3,x4 = x[:,:16], x[:,16:32], x[:,32:48], x[:,48:]\n",
    "        x1,x2 = x[:,:32], x[:,32:]\n",
    "\n",
    "        # results = self.attn_summary([x1,x2,x3,x4])\n",
    "        results = self.attn_summary([x1,x2])\n",
    "\n",
    "        return results\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([5, 64])\n",
      "tensor([[0.0000e+00, 1.0000e+00, 4.0000e+00, 9.0000e+00, 1.6000e+01, 2.5000e+01,\n",
      "         3.6000e+01, 4.9000e+01, 6.4000e+01, 8.1000e+01, 1.0000e+02, 1.2100e+02,\n",
      "         1.4400e+02, 1.6900e+02, 1.9600e+02, 2.2500e+02, 2.5600e+02, 2.8900e+02,\n",
      "         3.2400e+02, 3.6100e+02, 4.0000e+02, 4.4100e+02, 4.8400e+02, 5.2900e+02,\n",
      "         5.7600e+02, 6.2500e+02, 6.7600e+02, 7.2900e+02, 7.8400e+02, 8.4100e+02,\n",
      "         9.0000e+02, 9.6100e+02, 1.0240e+03, 1.0890e+03, 1.1560e+03, 1.2250e+03,\n",
      "         1.2960e+03, 1.3690e+03, 1.4440e+03, 1.5210e+03, 1.6000e+03, 1.6810e+03,\n",
      "         1.7640e+03, 1.8490e+03, 1.9360e+03, 2.0250e+03, 2.1160e+03, 2.2090e+03,\n",
      "         2.3040e+03, 2.4010e+03, 2.5000e+03, 2.6010e+03, 2.7040e+03, 2.8090e+03,\n",
      "         2.9160e+03, 3.0250e+03, 3.1360e+03, 3.2490e+03, 3.3640e+03, 3.4810e+03,\n",
      "         3.6000e+03, 3.7210e+03, 3.8440e+03, 3.9690e+03],\n",
      "        [0.0000e+00, 1.0000e+00, 4.0000e+00, 9.0000e+00, 1.6000e+01, 2.5000e+01,\n",
      "         3.6000e+01, 4.9000e+01, 6.4000e+01, 8.1000e+01, 1.0000e+02, 1.2100e+02,\n",
      "         1.4400e+02, 1.6900e+02, 1.9600e+02, 2.2500e+02, 2.5600e+02, 2.8900e+02,\n",
      "         3.2400e+02, 3.6100e+02, 4.0000e+02, 4.4100e+02, 4.8400e+02, 5.2900e+02,\n",
      "         5.7600e+02, 6.2500e+02, 6.7600e+02, 7.2900e+02, 7.8400e+02, 8.4100e+02,\n",
      "         9.0000e+02, 9.6100e+02, 1.0240e+03, 1.0890e+03, 1.1560e+03, 1.2250e+03,\n",
      "         1.2960e+03, 1.3690e+03, 1.4440e+03, 1.5210e+03, 1.6000e+03, 1.6810e+03,\n",
      "         1.7640e+03, 1.8490e+03, 1.9360e+03, 2.0250e+03, 2.1160e+03, 2.2090e+03,\n",
      "         2.3040e+03, 2.4010e+03, 2.5000e+03, 2.6010e+03, 2.7040e+03, 2.8090e+03,\n",
      "         2.9160e+03, 3.0250e+03, 3.1360e+03, 3.2490e+03, 3.3640e+03, 3.4810e+03,\n",
      "         3.6000e+03, 3.7210e+03, 3.8440e+03, 3.9690e+03],\n",
      "        [0.0000e+00, 1.0000e+00, 4.0000e+00, 9.0000e+00, 1.6000e+01, 2.5000e+01,\n",
      "         3.6000e+01, 4.9000e+01, 6.4000e+01, 8.1000e+01, 1.0000e+02, 1.2100e+02,\n",
      "         1.4400e+02, 1.6900e+02, 1.9600e+02, 2.2500e+02, 2.5600e+02, 2.8900e+02,\n",
      "         3.2400e+02, 3.6100e+02, 4.0000e+02, 4.4100e+02, 4.8400e+02, 5.2900e+02,\n",
      "         5.7600e+02, 6.2500e+02, 6.7600e+02, 7.2900e+02, 7.8400e+02, 8.4100e+02,\n",
      "         9.0000e+02, 9.6100e+02, 1.0240e+03, 1.0890e+03, 1.1560e+03, 1.2250e+03,\n",
      "         1.2960e+03, 1.3690e+03, 1.4440e+03, 1.5210e+03, 1.6000e+03, 1.6810e+03,\n",
      "         1.7640e+03, 1.8490e+03, 1.9360e+03, 2.0250e+03, 2.1160e+03, 2.2090e+03,\n",
      "         2.3040e+03, 2.4010e+03, 2.5000e+03, 2.6010e+03, 2.7040e+03, 2.8090e+03,\n",
      "         2.9160e+03, 3.0250e+03, 3.1360e+03, 3.2490e+03, 3.3640e+03, 3.4810e+03,\n",
      "         3.6000e+03, 3.7210e+03, 3.8440e+03, 3.9690e+03],\n",
      "        [0.0000e+00, 1.0000e+00, 4.0000e+00, 9.0000e+00, 1.6000e+01, 2.5000e+01,\n",
      "         3.6000e+01, 4.9000e+01, 6.4000e+01, 8.1000e+01, 1.0000e+02, 1.2100e+02,\n",
      "         1.4400e+02, 1.6900e+02, 1.9600e+02, 2.2500e+02, 2.5600e+02, 2.8900e+02,\n",
      "         3.2400e+02, 3.6100e+02, 4.0000e+02, 4.4100e+02, 4.8400e+02, 5.2900e+02,\n",
      "         5.7600e+02, 6.2500e+02, 6.7600e+02, 7.2900e+02, 7.8400e+02, 8.4100e+02,\n",
      "         9.0000e+02, 9.6100e+02, 1.0240e+03, 1.0890e+03, 1.1560e+03, 1.2250e+03,\n",
      "         1.2960e+03, 1.3690e+03, 1.4440e+03, 1.5210e+03, 1.6000e+03, 1.6810e+03,\n",
      "         1.7640e+03, 1.8490e+03, 1.9360e+03, 2.0250e+03, 2.1160e+03, 2.2090e+03,\n",
      "         2.3040e+03, 2.4010e+03, 2.5000e+03, 2.6010e+03, 2.7040e+03, 2.8090e+03,\n",
      "         2.9160e+03, 3.0250e+03, 3.1360e+03, 3.2490e+03, 3.3640e+03, 3.4810e+03,\n",
      "         3.6000e+03, 3.7210e+03, 3.8440e+03, 3.9690e+03],\n",
      "        [0.0000e+00, 1.0000e+00, 4.0000e+00, 9.0000e+00, 1.6000e+01, 2.5000e+01,\n",
      "         3.6000e+01, 4.9000e+01, 6.4000e+01, 8.1000e+01, 1.0000e+02, 1.2100e+02,\n",
      "         1.4400e+02, 1.6900e+02, 1.9600e+02, 2.2500e+02, 2.5600e+02, 2.8900e+02,\n",
      "         3.2400e+02, 3.6100e+02, 4.0000e+02, 4.4100e+02, 4.8400e+02, 5.2900e+02,\n",
      "         5.7600e+02, 6.2500e+02, 6.7600e+02, 7.2900e+02, 7.8400e+02, 8.4100e+02,\n",
      "         9.0000e+02, 9.6100e+02, 1.0240e+03, 1.0890e+03, 1.1560e+03, 1.2250e+03,\n",
      "         1.2960e+03, 1.3690e+03, 1.4440e+03, 1.5210e+03, 1.6000e+03, 1.6810e+03,\n",
      "         1.7640e+03, 1.8490e+03, 1.9360e+03, 2.0250e+03, 2.1160e+03, 2.2090e+03,\n",
      "         2.3040e+03, 2.4010e+03, 2.5000e+03, 2.6010e+03, 2.7040e+03, 2.8090e+03,\n",
      "         2.9160e+03, 3.0250e+03, 3.1360e+03, 3.2490e+03, 3.3640e+03, 3.4810e+03,\n",
      "         3.6000e+03, 3.7210e+03, 3.8440e+03, 3.9690e+03]])\n"
     ]
    }
   ],
   "source": [
    "a = multi_head()\n",
    "\n",
    "b = torch.FloatTensor([[i for i in range(64)] for j in range(5)])\n",
    "\n",
    "aa, bb = a(b)\n",
    "# print(b.shape)\n",
    "# print(aa.shape)\n",
    "# aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.tensor([[1, 1, 1, 1, 1],\n",
    "                             [2,2,2,2,2],\n",
    "                             [3,3,3,3,3],\n",
    "                             [1, 1, 1, 1, 1],\n",
    "                             [2,2,2,2,2],\n",
    "                             [3,3,3,3,3]])\n",
    "\n",
    "torch.cat((input_tensor[:3], input_tensor[3:]),axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grass2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
